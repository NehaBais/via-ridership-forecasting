{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> df[\"Latitude\"].min()\n",
    "36.974922\n",
    ">>> df[\"Latitude\"].max()\n",
    "37.558388\n",
    ">>> df[\"Longitude\"].min()\n",
    "-122.17364\n",
    ">>> df[\"Longitude\"].max()\n",
    "-121.54903\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_FOLDER = \"raw_data\"\n",
    "STAGING_DATA_FOLDER = \"staging_data\"\n",
    "ELEMENTS = [\"TMAX\", \"TMIN\", \"PRCP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we got this file from previous semester DB project.\n",
    "# NCDC CDO (Climate Data Online) website also provides a REST API to extract the same data.\n",
    "# filtering the lat and long which is within the bay area bounds.\n",
    "# filter only stations ids under bay area\n",
    "stations = (\n",
    "    pl.scan_csv(os.path.join(RAW_DATA_FOLDER, \"stations.csv\"))\n",
    "    .select([\"id\", \"latitude\", \"longitude\"])\n",
    "    .with_columns(\n",
    "        pl.col(\"latitude\").cast(pl.Float32), pl.col(\"longitude\").cast(pl.Float32)\n",
    "    )\n",
    "    .filter((pl.col(\"latitude\").ge(36.97)) & (pl.col(\"latitude\").le(37.56)))\n",
    "    .filter((pl.col(\"longitude\").ge(-122.18)) & (pl.col(\"longitude\").le(-121.54)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = (\n",
    "    pl.scan_csv(os.path.join(RAW_DATA_FOLDER, \"2014.csv\"))\n",
    "    .select([pl.col(\"ID\"), pl.col(\"DATE\"), pl.col(\"ELEMENT\"), pl.col(\"DATA_VALUE\")])\n",
    "    .with_columns(pl.col(\"DATE\").cast(pl.Utf8).str.strptime(pl.Date, \"%Y%m%d\"))\n",
    "    .with_columns((pl.col(\"DATA_VALUE\") / 10).cast(pl.Float32))\n",
    "    .filter(pl.col(\"ELEMENT\").is_in(ELEMENTS))\n",
    "    .filter(pl.col(\"ID\").is_in(stations.select(\"id\").collect()))\n",
    "    .unique([\"ID\", \"DATE\", \"ELEMENT\"])\n",
    "    .group_by([\"ID\", \"DATE\"])  # doing a pivot table\n",
    "    .agg(\n",
    "        pl.col(\"DATA_VALUE\")\n",
    "        .filter(pl.col(\"ELEMENT\").eq(element)) # making new columns tmax, tmin, prcp to add its data_values to it since element is categorical\n",
    "        .alias(element)\n",
    "        .mean()\n",
    "        for element in ELEMENTS\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example:\n",
    "\n",
    "| id    |   date        |   element |   datavalue   |\n",
    "| ----- | ------------- | --------- | ------------- |\n",
    "| 1     |   2014/1/1    |   tmax    |   30.0        |\n",
    "| 1     |   2014/1/1    |   tmin    |   12          |\n",
    "\n",
    "post group by and aggregate:\n",
    "\n",
    "| id    |   date        |   tmax    |   tmin    |   prcp    |\n",
    "| ----- | ------------- | --------- | --------- | ----------|\n",
    "| 1     |   2014/1/1    |   30.0    |   12      |   N/A     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 5)\n",
      "┌───────┬───────┬──────┬──────┬───────┐\n",
      "│ ID    ┆ DATE  ┆ TMAX ┆ TMIN ┆ PRCP  │\n",
      "│ ---   ┆ ---   ┆ ---  ┆ ---  ┆ ---   │\n",
      "│ u32   ┆ u32   ┆ u32  ┆ u32  ┆ u32   │\n",
      "╞═══════╪═══════╪══════╪══════╪═══════╡\n",
      "│ 15717 ┆ 15717 ┆ 6085 ┆ 6085 ┆ 13165 │\n",
      "└───────┴───────┴──────┴──────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(climate.count().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = (\n",
    "    pl.scan_csv(os.path.join(RAW_DATA_FOLDER, \"2015.csv\"))\n",
    "    .select([pl.col(\"ID\"), pl.col(\"DATE\"), pl.col(\"ELEMENT\"), pl.col(\"DATA_VALUE\")])\n",
    "    .with_columns(pl.col(\"DATE\").cast(pl.Utf8).str.strptime(pl.Date, \"%Y%m%d\"))\n",
    "    .with_columns((pl.col(\"DATA_VALUE\") / 10).cast(pl.Float32))\n",
    "    .filter(pl.col(\"ELEMENT\").is_in(ELEMENTS))\n",
    "    .filter(pl.col(\"ID\").is_in(stations.select(\"id\").collect()))\n",
    "    .unique([\"ID\", \"DATE\", \"ELEMENT\"])\n",
    "    .group_by([\"ID\", \"DATE\"])\n",
    "    .agg(\n",
    "        pl.col(\"DATA_VALUE\")\n",
    "        .filter(pl.col(\"ELEMENT\").eq(element))\n",
    "        .alias(element)\n",
    "        .mean()\n",
    "        for element in ELEMENTS\n",
    "    )\n",
    ")\n",
    "climate = pl.concat([climate, lf], how=\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 5)\n",
      "┌───────┬───────┬───────┬───────┬───────┐\n",
      "│ ID    ┆ DATE  ┆ TMAX  ┆ TMIN  ┆ PRCP  │\n",
      "│ ---   ┆ ---   ┆ ---   ┆ ---   ┆ ---   │\n",
      "│ u32   ┆ u32   ┆ u32   ┆ u32   ┆ u32   │\n",
      "╞═══════╪═══════╪═══════╪═══════╪═══════╡\n",
      "│ 31838 ┆ 31838 ┆ 12003 ┆ 11989 ┆ 26724 │\n",
      "└───────┴───────┴───────┴───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(climate.count().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = (\n",
    "    pl.scan_csv(os.path.join(RAW_DATA_FOLDER, \"2016.csv\"))\n",
    "    .select([pl.col(\"ID\"), pl.col(\"DATE\"), pl.col(\"ELEMENT\"), pl.col(\"DATA_VALUE\")])\n",
    "    .with_columns(pl.col(\"DATE\").cast(pl.Utf8).str.strptime(pl.Date, \"%Y%m%d\"))\n",
    "    .with_columns((pl.col(\"DATA_VALUE\") / 10).cast(pl.Float32))\n",
    "    .filter(pl.col(\"ELEMENT\").is_in(ELEMENTS))\n",
    "    .filter(pl.col(\"ID\").is_in(stations.select(\"id\").collect()))\n",
    "    .unique([\"ID\", \"DATE\", \"ELEMENT\"])\n",
    "    .group_by([\"ID\", \"DATE\"])\n",
    "    .agg(\n",
    "        pl.col(\"DATA_VALUE\")\n",
    "        .filter(pl.col(\"ELEMENT\").eq(element))\n",
    "        .alias(element)\n",
    "        .mean()\n",
    "        for element in ELEMENTS\n",
    "    )\n",
    ")\n",
    "climate = pl.concat([climate, lf], how=\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 5)\n",
      "┌───────┬───────┬───────┬───────┬───────┐\n",
      "│ ID    ┆ DATE  ┆ TMAX  ┆ TMIN  ┆ PRCP  │\n",
      "│ ---   ┆ ---   ┆ ---   ┆ ---   ┆ ---   │\n",
      "│ u32   ┆ u32   ┆ u32   ┆ u32   ┆ u32   │\n",
      "╞═══════╪═══════╪═══════╪═══════╪═══════╡\n",
      "│ 47861 ┆ 47861 ┆ 18005 ┆ 17969 ┆ 40193 │\n",
      "└───────┴───────┴───────┴───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(climate.count().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = (\n",
    "    pl.scan_csv(os.path.join(RAW_DATA_FOLDER, \"2017.csv\"))\n",
    "    .select([pl.col(\"ID\"), pl.col(\"DATE\"), pl.col(\"ELEMENT\"), pl.col(\"DATA_VALUE\")])\n",
    "    .with_columns(pl.col(\"DATE\").cast(pl.Utf8).str.strptime(pl.Date, \"%Y%m%d\"))\n",
    "    .with_columns((pl.col(\"DATA_VALUE\") / 10).cast(pl.Float32))\n",
    "    .filter(pl.col(\"ELEMENT\").is_in(ELEMENTS))\n",
    "    .filter(pl.col(\"ID\").is_in(stations.select(\"id\").collect()))\n",
    "    .unique([\"ID\", \"DATE\", \"ELEMENT\"])\n",
    "    .group_by([\"ID\", \"DATE\"])\n",
    "    .agg(\n",
    "        pl.col(\"DATA_VALUE\")\n",
    "        .filter(pl.col(\"ELEMENT\").eq(element))\n",
    "        .alias(element)\n",
    "        .mean()\n",
    "        for element in ELEMENTS\n",
    "    )\n",
    ")\n",
    "climate = pl.concat([climate, lf], how=\"vertical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 5)\n",
      "┌───────┬───────┬───────┬───────┬───────┐\n",
      "│ ID    ┆ DATE  ┆ TMAX  ┆ TMIN  ┆ PRCP  │\n",
      "│ ---   ┆ ---   ┆ ---   ┆ ---   ┆ ---   │\n",
      "│ u32   ┆ u32   ┆ u32   ┆ u32   ┆ u32   │\n",
      "╞═══════╪═══════╪═══════╪═══════╪═══════╡\n",
      "│ 63667 ┆ 63667 ┆ 23613 ┆ 23568 ┆ 53419 │\n",
      "└───────┴───────┴───────┴───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(climate.count().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the weather stations which have provided tmax data for more than 95% of the days so that the staions that are giving null vales in \n",
    "# element field will be removed.\n",
    "climate95 = (\n",
    "    climate.group_by(\"ID\")\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"DATE\").len(),\n",
    "            pl.col(\"TMAX\").null_count(),\n",
    "            pl.col(\"TMIN\").null_count(),\n",
    "            pl.col(\"PRCP\").null_count(),\n",
    "        ]\n",
    "    )\n",
    "    .filter((pl.col(\"PRCP\") / pl.col(\"DATE\")) < 0.05)\n",
    "    .filter((pl.col(\"TMAX\") / pl.col(\"DATE\")) < 0.05)\n",
    "    .select(\"ID\")\n",
    "    .collect()\n",
    ")\n",
    "climate = climate.filter(pl.col(\"ID\").is_in(climate95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 5)\n",
      "┌───────┬───────┬───────┬───────┬───────┐\n",
      "│ ID    ┆ DATE  ┆ TMAX  ┆ TMIN  ┆ PRCP  │\n",
      "│ ---   ┆ ---   ┆ ---   ┆ ---   ┆ ---   │\n",
      "│ u32   ┆ u32   ┆ u32   ┆ u32   ┆ u32   │\n",
      "╞═══════╪═══════╪═══════╪═══════╪═══════╡\n",
      "│ 13448 ┆ 13448 ┆ 13403 ┆ 13358 ┆ 13410 │\n",
      "└───────┴───────┴───────┴───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(climate.count().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = (\n",
    "    pl.scan_csv(os.path.join(STAGING_DATA_FOLDER, \"dates.csv\"))\n",
    "    .with_columns(\n",
    "        pl.col(\"id\").cast(pl.Utf8).str.strptime(pl.Date, \"%Y%m%d\").alias(\"DATE\")  # parsing date here\n",
    "    )\n",
    "    .select(pl.col(\"DATE\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the unique station ids and cross joining or applying cartisian product on dates\n",
    "unique_stations = climate.select(\"ID\").unique()\n",
    "unique_stations = unique_stations.join(dates, on=\"DATE\", how=\"cross\")\n",
    "\n",
    "# left join climate and unique station on columns id and date. just in case if we missed any dates in climate df\n",
    "climate = unique_stations.join(climate, on=[\"ID\", \"DATE\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 5)\n",
      "┌───────┬───────┬───────┬───────┬───────┐\n",
      "│ ID    ┆ DATE  ┆ TMAX  ┆ TMIN  ┆ PRCP  │\n",
      "│ ---   ┆ ---   ┆ ---   ┆ ---   ┆ ---   │\n",
      "│ u32   ┆ u32   ┆ u32   ┆ u32   ┆ u32   │\n",
      "╞═══════╪═══════╪═══════╪═══════╪═══════╡\n",
      "│ 14610 ┆ 14610 ┆ 13403 ┆ 13358 ┆ 13410 │\n",
      "└───────┴───────┴───────┴───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(climate.count().collect())\n",
    "# Actual no. of dates for each station available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_median(df, column_name):\n",
    "    # Calculate median per group\n",
    "    median_df = df.group_by([\"ID\", \"YEAR\", \"MONTH\"]).agg(\n",
    "        pl.col(column_name).median().alias(\"median_\" + column_name)\n",
    "    )\n",
    "    # Join back to the original data to align the medians\n",
    "    df = df.join(median_df, on=[\"ID\", \"YEAR\", \"MONTH\"])\n",
    "    # Impute the nulls with the corresponding median\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(column_name).is_null()) # if element value is null in original data\n",
    "        .then(pl.col(\"median_\" + column_name)) # impute with median element value for that station for that year and month\n",
    "        .otherwise(pl.col(column_name)) # if not null preserve the original value\n",
    "        .alias(column_name)\n",
    "    )\n",
    "    # Drop the median column after imputation\n",
    "    df = df.drop(\"median_\" + column_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = climate.with_columns(\n",
    "    pl.col(\"DATE\").dt.year().alias(\"YEAR\"), pl.col(\"DATE\").dt.month().alias(\"MONTH\")\n",
    ")\n",
    "climate = climate.pipe(impute_median, \"TMAX\")\n",
    "climate = climate.pipe(impute_median, \"TMIN\")\n",
    "climate = climate.pipe(impute_median, \"PRCP\")\n",
    "climate = climate.drop([\"YEAR\", \"MONTH\"])\n",
    "# pipe is a function that will apply a user function in our case impute median to all the rows of a data frame one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 5)\n",
      "┌───────┬───────┬───────┬───────┬───────┐\n",
      "│ ID    ┆ DATE  ┆ TMAX  ┆ TMIN  ┆ PRCP  │\n",
      "│ ---   ┆ ---   ┆ ---   ┆ ---   ┆ ---   │\n",
      "│ u32   ┆ u32   ┆ u32   ┆ u32   ┆ u32   │\n",
      "╞═══════╪═══════╪═══════╪═══════╪═══════╡\n",
      "│ 14610 ┆ 14610 ┆ 14338 ┆ 14338 ┆ 14338 │\n",
      "└───────┴───────┴───────┴───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(climate.count().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filling the rest null values with median because median is more insensitive to outliers\n",
    "climate = climate.with_columns(\n",
    "    pl.col(\"TMAX\").fill_null(pl.col(\"TMAX\").median()),\n",
    "    pl.col(\"TMIN\").fill_null(pl.col(\"TMIN\").median()),\n",
    "    pl.col(\"PRCP\").fill_null(0.0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 5)\n",
      "┌───────┬───────┬───────┬───────┬───────┐\n",
      "│ ID    ┆ DATE  ┆ TMAX  ┆ TMIN  ┆ PRCP  │\n",
      "│ ---   ┆ ---   ┆ ---   ┆ ---   ┆ ---   │\n",
      "│ u32   ┆ u32   ┆ u32   ┆ u32   ┆ u32   │\n",
      "╞═══════╪═══════╪═══════╪═══════╪═══════╡\n",
      "│ 14610 ┆ 14610 ┆ 14610 ┆ 14610 ┆ 14610 │\n",
      "└───────┴───────┴───────┴───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(climate.count().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joning stations and climate to get latitude and longitude\n",
    "climate = climate.join(stations, how=\"left\", left_on=\"ID\", right_on=\"id\").select(\n",
    "    [\n",
    "        pl.col(\"DATE\").alias(\"Date\"),\n",
    "        pl.col(\"latitude\").alias(\"Latitude\"),\n",
    "        pl.col(\"longitude\").alias(\"Longitude\"),\n",
    "        pl.col(\"TMAX\").alias(\"Tmax\"),\n",
    "        pl.col(\"TMIN\").alias(\"Tmin\"),\n",
    "        pl.col(\"PRCP\").alias(\"Prcp\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 6)\n",
      "┌───────┬──────────┬───────────┬───────┬───────┬───────┐\n",
      "│ Date  ┆ Latitude ┆ Longitude ┆ Tmax  ┆ Tmin  ┆ Prcp  │\n",
      "│ ---   ┆ ---      ┆ ---       ┆ ---   ┆ ---   ┆ ---   │\n",
      "│ u32   ┆ u32      ┆ u32       ┆ u32   ┆ u32   ┆ u32   │\n",
      "╞═══════╪══════════╪═══════════╪═══════╪═══════╪═══════╡\n",
      "│ 14610 ┆ 14610    ┆ 14610     ┆ 14610 ┆ 14610 ┆ 14610 │\n",
      "└───────┴──────────┴───────────┴───────┴───────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "print(climate.count().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate.collect(streaming=True).write_csv(\n",
    "    os.path.join(STAGING_DATA_FOLDER, \"climate.csv\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
